# -*- coding: utf-8 -*-
"""Asistente Sammy para Hotel - Notebook de Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PBsRbyUyk3cMY2paN76vht1ONCrUioVi

# Proyecto: Asistente Virtual "Sammy" para el Hotel Paraíso Caribeño

**Creado por: Grupo #3**

## 1. Introducción

Este proyecto consiste en el desarrollo de "Sammy", un asistente virtual inteligente diseñado para el Hotel Paraíso Caribeño, ubicado en Santa Marta. Sammy tiene como objetivo principal mejorar la experiencia del cliente al proporcionar información, gestionar consultas y ayudar con tareas como la verificación de usuarios, el registro y la creación de solicitudes de reserva.

El asistente ha sido desarrollado en un entorno de Google Colab, utilizando Python como lenguaje de programación principal y aprovechando las capacidades avanzadas del modelo de lenguaje grande (LLM) Gemini de Google para la comprensión del lenguaje natural y la generación de respuestas.

## 2. Objetivos del Proyecto

* Crear un asistente conversacional capaz de entender y responder a las solicitudes de los usuarios de manera natural.
* Integrar el asistente con una base de datos (MongoDB Atlas) para gestionar información de usuarios, paquetes, servicios y reservas.
* Implementar funcionalidades clave como:
    * Bienvenida y recolección de datos del usuario.
    * Verificación de usuarios existentes y registro de nuevos usuarios.
    * Consulta de información sobre paquetes de alojamiento, habitaciones y servicios/tours adicionales.
    * Creación de solicitudes de reserva, incluyendo la selección de paquetes y servicios.
    * Consulta del estado de reservas existentes.
    * Actualización del estado de las reservas.
* Proporcionar una interfaz de usuario interactiva dentro del notebook de Google Colab utilizando `ipywidgets` para facilitar la demostración y la interacción.
* Demostrar el uso de la técnica de "function calling" (uso de herramientas) con el LLM Gemini para realizar acciones específicas y obtener datos de fuentes externas (la base de datos).

## 3. Tecnologías Utilizadas

* **Lenguaje de Programación:** Python
* **Entorno de Desarrollo:** Google Colaboratory (Colab)
* **Modelo de Lenguaje (IA):** API de Gemini (específicamente, el modelo `gemini-1.5-flash-latest` o similar) a través de la librería `google-generativeai`.
* **Base de Datos:** MongoDB Atlas (NoSQL, servicio en la nube), interactuando mediante la librería `pymongo`.
* **Interfaz de Usuario en Colab:** `ipywidgets` para crear una experiencia de chat interactiva dentro del notebook.
* **Manejo de Credenciales:** `google.colab.userdata` para acceder a API keys de forma segura.
* **Otras Librerías Python:**
    * `os`: Para interactuar con el sistema operativo (no se usa extensivamente en la versión Colab, más relevante para el backend Flask).
    * `datetime`: Para manejar fechas y horas.
    * `uuid`: Para generar identificadores únicos.
    * `html`: Para escapar texto HTML en la interfaz de usuario.

## 4. Estructura del Notebook de Colab

El proyecto en Google Colab está organizado en varias celdas, cada una con una responsabilidad específica:

1.  **Celda 1: Instalación de Librerías:**
    * Instala las dependencias necesarias (`google-generativeai`, `pymongo`, `ipywidgets`).
2.  **Celda 2: Importaciones Esenciales y Configuración Inicial:**
    * Importa todos los módulos de Python requeridos.
    * Define variables globales para la configuración (URIs de conexión, nombres de colecciones, API keys leídas desde los Secrets de Colab).
    * Inicializa variables globales para los objetos de base de datos y el modelo de IA.
    * Define el `DEBUG_MODE` para controlar la verbosidad de los logs.
3.  **Celda 3: Función Auxiliar de Serialización:**
    * Contiene la función `_serializar_documento_para_llm`, que es crucial para convertir los datos de MongoDB (especialmente `ObjectId` y `datetime`) a un formato que el LLM pueda procesar sin errores.
4.  **Celda 4: Definición de Funciones Herramienta para Sammy:**
    * Aquí se definen todas las funciones Python que Sammy (el LLM) puede "llamar". Estas funciones son las "habilidades" de Sammy y le permiten interactuar con la base de datos o realizar cálculos. Ejemplos: `Bienvenida_cliente`, `verificar_o_registrar_usuario`, `listar_paquetes_disponibles`, `crear_reserva`, etc.
    * Se incluye la lista `herramientas_disponibles_colab` que agrupa todas estas funciones para pasarlas al modelo Gemini.
5.  **Celda 5: Función de Inicialización Principal (`inicializar_app_colab`):**
    * Esta función se encarga de:
        * Establecer la conexión con MongoDB Atlas.
        * Configurar el cliente de Gemini con la API Key.
        * Crear la instancia del `GenerativeModel` de Gemini, pasándole las `herramientas_disponibles_colab` y la `system_instruction` (el prompt de sistema que define la personalidad y el comportamiento de Sammy).
        * Iniciar la sesión de chat (`chat_session_sammy`).
6.  **Celda 6: Función para Procesar Mensajes (`procesar_mensaje_colab`):**
    * Esta es la función central que maneja cada turno de la conversación.
    * Toma el mensaje del usuario.
    * Implementa la lógica para mapear selecciones numéricas del usuario a IDs de ítems (si Sammy acaba de presentar una lista).
    * Envía el mensaje al `chat_session_sammy`.
    * Maneja el bucle de "function calling": si el LLM decide usar una herramienta, esta función la ejecuta y envía el resultado de vuelta al LLM.
    * Extrae la respuesta final de texto del LLM para mostrarla al usuario.
7.  **Celda 7: Interfaz Gráfica con `ipywidgets` y Lógica de Chat:**
    * Define los estilos HTML para la interfaz de chat.
    * Crea los widgets: un área para mostrar la conversación, un campo de texto para la entrada del usuario y un botón de envío.
    * Implementa la lógica de la UI (`add_message_to_ui`, `on_send_button_clicked_ui`) para mostrar la conversación y manejar las interacciones del usuario.
    * Llama a `inicializar_app_colab()` y, si tiene éxito, muestra la interfaz de chat e inicia la primera interacción con Sammy.

## 5. Funcionamiento del Asistente (Interacción con Gemini)

El núcleo de la inteligencia de Sammy reside en el modelo Gemini y su capacidad de "function calling":

1.  **Prompt de Sistema (`system_instruction`):** Al inicializar el modelo Gemini, se le proporciona una instrucción de sistema detallada. Esta instrucción define la personalidad de Sammy (amigable, profesional), su rol (asistente del Hotel Paraíso Caribeño), sus objetivos, y, muy importante, cómo y cuándo debe utilizar las diferentes funciones herramienta que se le han proporcionado.
2.  **Mensaje del Usuario:** Cuando el usuario envía un mensaje, este se pasa al modelo Gemini.
3.  **Decisión del LLM:** Basándose en el mensaje del usuario, el historial de la conversación y la instrucción de sistema, el LLM decide cómo responder. Puede:
    * Responder directamente con texto.
    * Decidir que necesita más información o realizar una acción, para lo cual "llama" a una de las funciones herramienta definidas en Python.
4.  **Ejecución de la Herramienta (Python):** Si el LLM decide llamar a una herramienta, el código Python (en `procesar_mensaje_colab`) recibe el nombre de la función y los argumentos que el LLM ha extraído de la conversación. El código Python ejecuta la función correspondiente (ej., una consulta a MongoDB).
5.  **Resultado de la Herramienta al LLM:** El resultado de la función Python (generalmente un diccionario con un `status` y datos) se envía de vuelta al LLM.
6.  **Respuesta Final del LLM:** Con la información obtenida de la herramienta (o si no necesitó una), el LLM formula una respuesta en lenguaje natural para el usuario. El `system_instruction` le indica explícitamente que NO muestre el JSON crudo de los `tool_outputs`.

Este ciclo se repite para cada turno de la conversación.

## 6. Cómo Usar el Notebook de Colab

1.  **Configurar Secrets:** Antes de ejecutar, asegúrate de haber configurado las siguientes variables en los "Secrets" de Google Colab (icono de llave 🔑 en el panel izquierdo):
    * `MONGODB_ATLAS_URI`: Tu cadena de conexión a MongoDB Atlas.
    * `GEMINI_API_KEY`: Tu clave API para los modelos Gemini de Google AI Studio.
2.  **Ejecutar Celdas en Orden:** Es crucial ejecutar todas las celdas del notebook en orden, desde la Celda 1 hasta la Celda 7.
    * La Celda 1 instala las librerías.
    * Las Celdas 2-6 definen todas las configuraciones, funciones y lógica necesarias.
    * La Celda 7 es la que finalmente inicializa la aplicación y muestra la interfaz de chat interactiva.
3.  **Interactuar con Sammy:** Una vez que la Celda 7 se ejecuta y muestra el mensaje "¡Todo listo!...", podrás usar la interfaz de chat (el campo de texto y el botón "Enviar") para hablar con Sammy.
4.  **Modo de Depuración (`DEBUG_MODE`):** En la Celda 2, puedes cambiar `DEBUG_MODE = True` si deseas ver los logs internos de las llamadas a funciones y sus resultados en la salida de la celda (debajo de la interfaz de chat). Para una presentación limpia, mantenlo en `False`.
5.  **Finalizar:** Escribe "salir" en el chat para terminar la conversación. La conexión a MongoDB se cerrará si se implementa un botón o lógica para ello, o al finalizar la sesión de Colab.

## 7. Posibles Mejoras Futuras

* Implementación de una lógica de disponibilidad de habitaciones más robusta.
* Cálculo de precios finales más preciso (considerando temporadas, ofertas, etc.).
* Gestión de sesiones de chat más avanzada si se llevara a un entorno multiusuario.
* Integración con un frontend web externo (React, Vue) y un backend Flask/FastAPI desplegado en la nube.
* Añadir más herramientas para Sammy (ej. modificar reservas, obtener información turística de Santa Marta, etc.).

---

Este proyecto demuestra una aplicación práctica de los modelos de lenguaje grandes con capacidad de uso de herramientas para crear asistentes virtuales funcionales y útiles.
"""

# Celda 1: Instalación de Librerías
# ---------------------------------------------------------------------------
# Primero, me aseguro de que todas las librerías que necesito estén instaladas
# en el entorno de Colab. El -q hace que la instalación sea silenciosa.
!pip install -q google-generativeai pymongo python-dotenv ipywidgets

# Celda 2: Importaciones Esenciales y Configuración Inicial
# ---------------------------------------------------------------------------
print("Colab Notebook: Cargando módulos y configuraciones iniciales...")

import os
import pymongo # Para la base de datos MongoDB
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from datetime import datetime # Para manejar fechas y horas
import uuid # Para generar IDs únicos para las reservas
import google.generativeai as genai # La librería de Gemini
from google.ai import generativelanguage as glm # Para construir el FunctionResponse
from google.colab import userdata # Para obtener mis API keys de los Secrets de Colab
import html # Para escapar HTML en la UI y prevenir XSS básico

# Importaciones para la Interfaz Gráfica con ipywidgets
import ipywidgets as widgets
from IPython.display import display, HTML

# --- Configuración de Mis Variables Globales ---
# Obtengo mis credenciales de los Secrets de Colab.
# Es importante tener MONGODB_ATLAS_URI y GEMINI_API_KEY configurados allí.
MONGODB_URI = userdata.get('MONGODB_ATLAS_URI')
GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')

# Nombres que he decidido para mi base de datos y colecciones.
DB_NAME_PRINCIPAL = "TalentoTech"
COLL_USUARIOS_NAME = "usuarios"
COLL_SERVICIOS_NAME = "servicios"
COLL_PAQUETES_NAME = "paquetes"
COLL_HABITACIONES_NAME = "habitaciones"
COLL_RESERVAS_NAME = "reservas"

# Variables que contendrán los objetos de conexión y el modelo de IA.
# Las inicializo a None; la función de inicialización se encargará de poblarlas.
client_mongo = None
db = None
usuarios_coll, servicios_coll, paquetes_coll, habitaciones_coll, reservas_coll = None, None, None, None, None
model_gemini = None
chat_session_sammy = None

# Estado de la sesión de chat: para recordar cosas simples durante la conversación.
session_state = {"bienvenida_dada": False, "ultima_lista_presentada": None, "usuario_identificado": None}

# DEBUG_MODE: Si es True, se mostrarán prints detallados de depuración.
# Para la presentación, lo dejo en False para una interfaz más limpia.
DEBUG_MODE = False

print("Colab Notebook: Variables globales listas.")

# Celda 3: Función Auxiliar de Serialización de Datos
# ---------------------------------------------------------------------------
# Esta función me ayuda a preparar los datos que vienen de MongoDB
# para que el LLM los pueda entender bien (convirtiendo tipos especiales a strings).
def _serializar_documento_para_llm(documento: dict) -> dict:
    """
    Prepara una copia de un documento de MongoDB para ser enviado al LLM.
    Convierte ObjectId a string y objetos datetime a strings formateados (YYYY-MM-DD HH:MM:SS).
    Elimina los campos datetime originales después de crear su versión string.
    """
    if documento is None: # Manejo el caso de que el documento sea nulo.
        return {}

    doc_copia = dict(documento) # Trabajo sobre una copia para no alterar el original.

    # El _id de MongoDB es un ObjectId, lo convierto a string.
    if '_id' in doc_copia and not isinstance(doc_copia['_id'], str):
        doc_copia['_id'] = str(doc_copia['_id'])

    # Busco todos los campos que sean de tipo datetime.
    campos_fecha_a_procesar = [key for key, value in doc_copia.items() if isinstance(value, datetime)]

    # Convierto cada campo de fecha a un string formateado y elimino el original.
    for field in campos_fecha_a_procesar:
        doc_copia[f"{field}_str"] = doc_copia[field].strftime('%Y-%m-%d %H:%M:%S')
        del doc_copia[field] # Importante para evitar errores de serialización con el LLM.

    return doc_copia

# Celda 4: Definición de Funciones Herramienta para Sammy
# ---------------------------------------------------------------------------
# Estas son las funciones que le dan "poderes" a Sammy para interactuar con mi base de datos
# y realizar las tareas que necesito. El LLM decidirá cuándo llamar a cada una.

def debug_print(mensaje: str):
    """Función auxiliar para imprimir mensajes de depuración solo si DEBUG_MODE es True."""
    if DEBUG_MODE:
        print(mensaje)

def Bienvenida_cliente() -> dict:
    """
    Define el saludo inicial de Sammy y cómo debe proceder para identificar al usuario.
    El LLM usará el 'mensaje_bienvenida_llm' para hablar con el usuario.
    """
    global session_state
    debug_print("⚙️ Colab/Sammy: LLM llamó a Bienvenida_cliente()")

    # Evito que Sammy salude repetidamente si ya lo hizo y el usuario está identificado.
    if session_state.get("bienvenida_dada") and session_state.get("usuario_identificado"):
        debug_print("ℹ️ Colab/Sammy: Bienvenida ya dada y usuario identificado.")
        nombre_usuario = session_state["usuario_identificado"].get("nombre_usuario", "Cliente")
        return {
            "mensaje_bienvenida_llm": f"¡Hola de nuevo, {nombre_usuario}! ¿Cómo puedo ayudarte hoy?",
            "proceder_directo": True
        }
    # Si ya saludó pero aún no identifica al usuario.
    elif session_state.get("bienvenida_dada"):
        debug_print("ℹ️ Colab/Sammy: Bienvenida ya dada, pero usuario no identificado.")
        return {
            "mensaje_bienvenida_llm": "Para continuar, por favor, indícame tu nombre completo y número de documento.",
            "campos_requeridos_secuencia": ["nombre_completo", "numero_documento"],
            "instruccion_para_llm_despues_de_obtener_datos": "Usa 'verificar_o_registrar_usuario'.",
            "siguiente_herramienta_sugerida": "verificar_o_registrar_usuario"
        }

    # El primer saludo.
    mensaje_para_el_usuario = (
        "¡Hola! 👋 Bienvenido al sistema de asistencia del Hotel Paraíso Caribeño en la hermosa Santa Marta. "
        "Soy Sammy, tu asistente virtual. Para comenzar y poder ayudarte mejor, ¿podrías decirme tu nombre completo y luego tu número de documento, por favor?"
    )
    session_state["bienvenida_dada"] = True
    return {
        "mensaje_bienvenida_llm": mensaje_para_el_usuario,
        "campos_requeridos_secuencia": ["nombre_completo", "numero_documento"],
        "instruccion_para_llm_despues_de_obtener_datos": "Una vez que tengas el nombre y el número de documento, utiliza la herramienta 'verificar_o_registrar_usuario'.",
        "siguiente_herramienta_sugerida": "verificar_o_registrar_usuario"
    }

def verificar_o_registrar_usuario(numero_documento: str, nombre_completo: str = None) -> dict:
    """
    Busca al usuario en la base de datos. Si existe, devuelve sus datos.
    Si no, guía al LLM para pedir el nombre (si aún no lo tiene) o para sugerir el registro.
    """
    global session_state
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a verificar_o_registrar_usuario(doc='{numero_documento}', nombre='{nombre_completo}')")
    if usuarios_coll is None: # Verifico si la conexión a la colección de usuarios está activa.
        return {"status": "error_conexion_db", "mensaje_para_llm": "Lo siento, no puedo acceder a la base de datos de usuarios en este momento."}
    try:
        usuario_encontrado_raw = usuarios_coll.find_one({"numero_documento": numero_documento})
        if usuario_encontrado_raw:
            usuario_serializado = _serializar_documento_para_llm(usuario_encontrado_raw)
            nombre_display = usuario_serializado.get('nombre_usuario') or usuario_serializado.get('nombrre_usuario', 'Cliente') # Manejo el typo 'nombre'
            session_state["usuario_identificado"] = usuario_serializado # Guardo el usuario en el estado de la sesión.
            return {
                "status": "usuario_encontrado", "datos_usuario": usuario_serializado,
                "mensaje_para_llm": f"¡Hola de nuevo, {nombre_display}! Ya te tengo en mis registros. ¿En qué te puedo ayudar hoy?"
            }
        else: # Si el usuario no se encuentra en la base de datos.
            if nombre_completo: # Si el LLM ya me pasó el nombre completo.
                return {
                    "status": "usuario_no_encontrado_proceder_registro",
                    "numero_documento_a_registrar": numero_documento, "nombre_completo_a_registrar": nombre_completo,
                    "mensaje_para_llm": (
                        f"Entendido, {nombre_completo}. Como eres nuevo, voy a registrarte con el documento '{numero_documento}'. "
                        "(LLM: Llama a `registrar_nuevo_usuario` con estos datos)." # Instrucción para el LLM.
                    ),
                    "siguiente_herramienta_sugerida": "registrar_nuevo_usuario",
                    "argumentos_siguiente_herramienta": {"nombre_completo": nombre_completo, "numero_documento": numero_documento}
                }
            else: # Si solo tengo el documento y necesito el nombre para registrar.
                return {
                    "status": "usuario_no_encontrado_pedir_nombre", "numero_documento_verificado": numero_documento,
                    "mensaje_para_llm": (
                        f"Parece que el número de documento '{numero_documento}' no está registrado. "
                        "Para continuar, ¿podrías por favor indicarme tu nombre completo para registrarte?"
                    )
                }
    except Exception as e:
        debug_print(f"🔴 Error en verificar_o_registrar_usuario: {str(e)}")
        return {"status": "error_busqueda_db", "mensaje_para_llm": "Tuve un problema técnico verificando el documento."}

def registrar_nuevo_usuario(nombre_completo: str, numero_documento: str, rol: str = "cliente") -> dict:
    """ Registra un nuevo usuario en la base de datos. """
    global session_state
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a registrar_nuevo_usuario(nombre='{nombre_completo}', doc='{numero_documento}')")
    if usuarios_coll is None:
        return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo registrar usuarios ahora (DB)." }
    try:
        # Verifico de nuevo por si acaso el flujo del LLM lo llama aunque ya exista.
        if usuarios_coll.find_one({"numero_documento": numero_documento}):
            usuario_existente_raw = usuarios_coll.find_one({"numero_documento": numero_documento})
            usuario_existente_serializado = _serializar_documento_para_llm(usuario_existente_raw or {})
            session_state["usuario_identificado"] = usuario_existente_serializado
            return {"status": "usuario_ya_existia", "datos_usuario": usuario_existente_serializado,
                    "mensaje_para_llm": f"El usuario {nombre_completo} con documento {numero_documento} ya estaba registrado. Puedes proceder con su solicitud."}

        nuevo_usuario_doc = {
            "numero_documento": numero_documento,
            "nombre_usuario": nombre_completo, # Aseguro consistencia en el nombre del campo.
            "numero_reservas_realizadas": 0,
            "rol": rol,
            "fecha_registro": datetime.now()
        }
        resultado = usuarios_coll.insert_one(dict(nuevo_usuario_doc)) # Guardo el nuevo usuario.

        # Obtengo el documento recién insertado para devolverlo completo y serializado.
        usuario_insertado_confirmacion_raw = usuarios_coll.find_one({"_id": resultado.inserted_id})
        usuario_insertado_serializado = _serializar_documento_para_llm(usuario_insertado_confirmacion_raw or {})
        session_state["usuario_identificado"] = usuario_insertado_serializado # Guardo el nuevo usuario en el estado.

        return {
            "status": "usuario_registrado_exitosamente", "datos_usuario_nuevo": usuario_insertado_serializado,
            "mensaje_para_llm": (f"¡Perfecto, {nombre_completo}! Tu registro con el número de documento {numero_documento} fue exitoso. "
                                 "Ahora, ¿cómo puedo ayudarte con tu reserva o alguna otra consulta?")
        }
    except Exception as e:
        debug_print(f"🔴 Error en registrar_nuevo_usuario: {str(e)}")
        return {"status": "error_registro_db", "mensaje_para_llm": f"Problema técnico al registrar a {nombre_completo}."}

def listar_servicios_disponibles(tipo_servicio: str = None) -> dict:
    """
    Lista los servicios y tours, opcionalmente filtrados por tipo.
    Guarda la lista en session_state para que el usuario pueda seleccionar por número.
    """
    global session_state
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a listar_servicios_disponibles(tipo_servicio='{tipo_servicio}')")
    if servicios_coll is None:
        return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo mostrar los servicios en este momento (DB)."}
    try:
        query = {}
        # Solo filtro si tipo_servicio tiene un valor y no es la cadena 'none' (que a veces manda el LLM).
        if tipo_servicio and tipo_servicio.lower() != 'none' and tipo_servicio.strip() != "":
            query["tipo_servicio"] = {"$regex": tipo_servicio.strip(), "$options": "i"} # Búsqueda flexible sin importar mayúsculas.

        # Pido solo los campos que necesito para mostrar al usuario y para la lógica.
        servicios_encontrados = list(servicios_coll.find(query, {"_id": 0, "id_servicio": 1, "nombre_servicio": 1, "descripcion_servicio": 1, "precio_adicional_servicio": 1, "tipo_servicio": 1}))

        if not servicios_encontrados:
            mensaje = f"No encontré servicios o tours del tipo '{tipo_servicio}'." if tipo_servicio and tipo_servicio.lower() != 'none' and tipo_servicio.strip() != "" else "No hay servicios adicionales o tours registrados en este momento."
            session_state["ultima_lista_presentada"] = None # Limpio la última lista si no hay resultados.
            return {"status": "servicios_no_encontrados", "mensaje_para_llm": mensaje}

        session_state["ultima_lista_presentada"] = {"tipo": "servicios", "items": servicios_encontrados}
        return {
            "status": "servicios_encontrados",
            "lista_servicios": servicios_encontrados,
            "mensaje_para_llm": (
                "Aquí tienes una lista de servicios y tours disponibles. Recuerda que los precios indicados son por persona. "
                "Puedes decirme el NÚMERO de la opción que te interesa (empezando por 1) o su ID (id_servicio) para añadirlo a tu reserva, o si quieres ver otro tipo."
            )
        }
    except Exception as e:
        debug_print(f"🔴 Error en listar_servicios_disponibles: {str(e)}")
        return {"status": "error_busqueda_db", "mensaje_para_llm": "Problema técnico al listar los servicios."}

def listar_paquetes_disponibles() -> dict:
    """
    Lista los paquetes de alojamiento.
    Guarda la lista en session_state para selección numérica.
    """
    global session_state
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a listar_paquetes_disponibles()")
    if paquetes_coll is None:
        return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo mostrar los paquetes en este momento (DB)."}
    try:
        # Asumo que en mi DB, los paquetes tienen 'precio_base_paquete_por_noche_persona'.
        paquetes_encontrados = list(paquetes_coll.find({}, {"_id": 0, "id_paquete": 1, "nombre_paquete": 1, "descripcion_paquete": 1, "precio_base_paquete_por_noche_persona": 1, "tipo_logica_paquete": 1}))

        if not paquetes_encontrados:
            session_state["ultima_lista_presentada"] = None
            return {"status": "paquetes_no_encontrados", "mensaje_para_llm": "No hay paquetes de alojamiento registrados en este momento."}

        session_state["ultima_lista_presentada"] = {"tipo": "paquetes", "items": paquetes_encontrados}
        return {
            "status": "paquetes_encontrados",
            "lista_paquetes": paquetes_encontrados,
            "mensaje_para_llm": "Estos son nuestros paquetes de alojamiento. Puedes decirme el NÚMERO del paquete que te interesa (empezando por 1) o su ID (id_paquete) para ver más detalles o seleccionarlo."
        }
    except Exception as e:
        debug_print(f"🔴 Error en listar_paquetes_disponibles: {str(e)}")
        return {"status": "error_busqueda_db", "mensaje_para_llm": "Problema técnico al listar los paquetes."}

def listar_reservas_por_usuario(numero_documento_usuario: str) -> dict:
    """Busca todas las reservas de un usuario."""
    global session_state
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a listar_reservas_por_usuario(doc='{numero_documento_usuario}')")

    # Si el LLM no pasa el documento pero el usuario ya está identificado en la sesión, lo uso.
    if not numero_documento_usuario and session_state.get("usuario_identificado"):
        numero_documento_usuario = session_state["usuario_identificado"].get("numero_documento")

    if reservas_coll is None or not numero_documento_usuario:
        return {"status": "error_datos_insuficientes", "mensaje_para_llm": "Necesito tu número de documento para buscar tus reservas. Si ya me lo diste, por favor confírmamelo."}
    try:
        reservas_raw = list(reservas_coll.find({"numero_documento_usuario": numero_documento_usuario}))
        if not reservas_raw:
            return {"status": "reservas_no_encontradas", "mensaje_para_llm": "No encontré reservas asociadas a tu número de documento."}

        reservas_serializadas = [_serializar_documento_para_llm(r) for r in reservas_raw]
        session_state["ultima_lista_presentada"] = {"tipo": "reservas", "items": reservas_serializadas}
        return {
            "status": "reservas_encontradas", "lista_reservas": reservas_serializadas,
            "mensaje_para_llm": "Aquí están las reservas que encontré para ti. Puedes pedirme detalles de una específica usando su NÚMERO en la lista o su ID de reserva."
        }
    except Exception as e:
        debug_print(f"🔴 Error en listar_reservas_por_usuario: {str(e)}")
        return {"status": "error_busqueda_db", "mensaje_para_llm": "Problema técnico buscando tus reservas."}

def crear_reserva(
    numero_documento_usuario: str,
    id_paquete_reservado: str,
    fecha_inicio_estadia_str: str,
    fecha_fin_estadia_str: str,
    numero_huespedes: int,
    ids_servicios_solicitados: list[str] = None
) -> dict:
    """
    Crea una solicitud de reserva, incluyendo servicios adicionales y un desglose de costos.
    """
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a crear_reserva para doc: {numero_documento_usuario}, paquete: {id_paquete_reservado}, servicios: {ids_servicios_solicitados}")
    if not all([usuarios_coll, paquetes_coll, reservas_coll, servicios_coll]): # Chequeo conciso de colecciones.
        return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo procesar reservas ahora (DB)."}
    try:
        # --- Validaciones de Entrada ---
        fecha_inicio = datetime.strptime(fecha_inicio_estadia_str, "%Y-%m-%d")
        fecha_fin = datetime.strptime(fecha_fin_estadia_str, "%Y-%m-%d")
        if fecha_fin <= fecha_inicio: return {"status": "error_validacion", "mensaje_para_llm": "La fecha de fin de estadía debe ser posterior a la fecha de inicio."}
        if fecha_inicio < datetime.now().replace(hour=0, minute=0, second=0, microsecond=0): # Comparar solo fechas.
             return {"status": "error_validacion", "mensaje_para_llm": "La fecha de inicio de la estadía no puede ser en el pasado."}
        if numero_huespedes <= 0:
            return {"status": "error_validacion", "mensaje_para_llm": "El número de huéspedes debe ser al menos 1."}

        # --- Obtener datos de Usuario y Paquete ---
        usuario = usuarios_coll.find_one({"numero_documento": numero_documento_usuario})
        if not usuario: return {"status": "error_usuario_no_encontrado", "mensaje_para_llm": "Usuario no encontrado para la reserva."}

        paquete = paquetes_coll.find_one({"id_paquete": id_paquete_reservado})
        if not paquete: return {"status": "error_paquete_no_encontrado", "mensaje_para_llm": "Paquete no encontrado."}

        # --- Procesar Servicios Adicionales y Calcular sus Costos ---
        servicios_detalle_reserva = []
        costo_total_servicios_por_persona_acumulado = 0
        if ids_servicios_solicitados:
            for id_serv in ids_servicios_solicitados:
                servicio_info = servicios_coll.find_one({"id_servicio": id_serv})
                if servicio_info:
                    precio_sp = servicio_info.get("precio_adicional_servicio", 0)
                    costo_total_servicios_por_persona_acumulado += precio_sp
                    servicios_detalle_reserva.append({
                        "id_servicio": id_serv,
                        "nombre_servicio": servicio_info.get("nombre_servicio"),
                        "precio_por_persona": precio_sp,
                        "costo_estimado_para_grupo": precio_sp * numero_huespedes
                    })
                else:
                    return {"status": "error_validacion", "mensaje_para_llm": f"El servicio con ID '{id_serv}' no es válido."}

        # --- Calcular Costos de Alojamiento y Totales ---
        noches = (fecha_fin - fecha_inicio).days
        if noches <= 0: return {"status": "error_validacion", "mensaje_para_llm": "La estadía debe ser de al menos una noche."}

        precio_paquete_noche_pp = paquete.get("precio_base_paquete_por_noche_persona", 0)
        if precio_paquete_noche_pp == 0: # Advertencia si el precio del paquete es 0.
            debug_print(f"⚠️ Advertencia: El paquete {id_paquete_reservado} tiene precio_base_paquete_por_noche_persona en 0 o no definido.")

        costo_alojamiento_total_grupo = precio_paquete_noche_pp * noches * numero_huespedes
        costo_servicios_adicionales_total_grupo = sum(s['costo_estimado_para_grupo'] for s in servicios_detalle_reserva)
        costo_total_estimado_reserva = costo_alojamiento_total_grupo + costo_servicios_adicionales_total_grupo

        # --- Construir y Guardar el Documento de Reserva ---
        id_nueva_reserva = f"SMRSV-{str(uuid.uuid4())[:8].upper()}"
        documento_reserva = {
            "id_reserva": id_nueva_reserva, "numero_documento_usuario": numero_documento_usuario,
            "nombre_cliente_reserva": usuario.get("nombre_usuario") or usuario.get("nombre_usuario"),
            "id_paquete_reservado": id_paquete_reservado, "nombre_paquete_reservado": paquete.get("nombre_paquete"),
            "fecha_solicitud_reserva": datetime.now(), "fecha_inicio_estadia": fecha_inicio,
            "fecha_fin_estadia": fecha_fin, "numero_huespedes": numero_huespedes, "numero_noches": noches,
            "servicios_adicionales_solicitados": servicios_detalle_reserva,
            "desglose_costos_estimados": {
                "alojamiento_por_noche_persona": precio_paquete_noche_pp,
                "alojamiento_total_grupo": costo_alojamiento_total_grupo,
                "servicios_adicionales_total_grupo": costo_servicios_adicionales_total_grupo,
                "gran_total_estimado": costo_total_estimado_reserva
            },
            "precio_final_pagado": None, "estado_precio": "pendiente_calculo_y_confirmacion",
            "estado_reserva": "solicitada", "notas_adicionales_cliente": ""
        }
        reservas_coll.insert_one(dict(documento_reserva))
        usuarios_coll.update_one({"numero_documento": numero_documento_usuario}, {"$inc": {"numero_reservas_realizadas": 1}})

        reserva_info_llm = _serializar_documento_para_llm(documento_reserva)

        # --- Preparar Mensaje de Confirmación para el LLM ---
        desglose_txt = (
            f"Desglose Estimado:\n"
            f"- Alojamiento ({paquete.get('nombre_paquete')} por {noches} noches para {numero_huespedes} personas): {costo_alojamiento_total_grupo:,.0f} COP.\n"
        )
        if servicios_detalle_reserva:
            serv_txt_list = [f"  - {s['nombre_servicio']}: {s['costo_estimado_para_grupo']:,.0f} COP para el grupo ({s['precio_por_persona']:,.0f} COP p/p)." for s in servicios_detalle_reserva]
            desglose_txt += f"- Servicios Adicionales:\n" + "\n".join(serv_txt_list) + "\n"
        desglose_txt += f"- TOTAL ESTIMADO RESERVA: {costo_total_estimado_reserva:,.0f} COP."

        return {
            "status": "reserva_solicitada_exitosamente", "datos_reserva_creada": reserva_info_llm,
            "mensaje_para_llm": (
                f"¡Solicitud de reserva '{id_nueva_reserva}' registrada para {documento_reserva['nombre_cliente_reserva']}!\n"
                f"{desglose_txt}\n"
                "Un agente confirmará disponibilidad y precio final pronto. ¿Algo más?"
            )
        }
    except ValueError: # Específicamente para errores de formato de fecha.
         return {"status": "error_validacion", "mensaje_para_llm": "El formato de las fechas no es válido. Por favor, usa AAAA-MM-DD."}
    except Exception as e:
        debug_print(f"🔴 Error en crear_reserva: {str(e)}")
        return {"status": "error_creacion_reserva_db", "mensaje_para_llm": "Problema técnico al crear la solicitud de reserva."}

def obtener_detalle_paquete(id_paquete: str) -> dict:
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a obtener_detalle_paquete(id_paquete='{id_paquete}')")
    if paquetes_coll is None: return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo ver paquetes ahora (DB)."}
    try:
        paquete_raw = paquetes_coll.find_one({"id_paquete": id_paquete})
        if paquete_raw:
            paquete_serializado = _serializar_documento_para_llm(paquete_raw)
            return {"status": "paquete_encontrado", "datos_paquete": paquete_serializado, "mensaje_para_llm": f"Detalles del paquete '{paquete_serializado.get('nombre_paquete', id_paquete)}' listos."}
        return {"status": "paquete_no_encontrado", "mensaje_para_llm": f"Paquete ID '{id_paquete}' no hallado."}
    except Exception as e:
        debug_print(f"🔴 Error en obtener_detalle_paquete: {str(e)}")
        return {"status": "error_busqueda_db", "mensaje_para_llm": "Problema técnico buscando paquete."}

def obtener_detalle_habitacion(id_habitacion: str) -> dict:
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a obtener_detalle_habitacion(id_habitacion='{id_habitacion}')")
    if habitaciones_coll is None: return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo ver habitaciones ahora (DB)."}
    try:
        habitacion_raw = habitaciones_coll.find_one({"id_habitacion": id_habitacion})
        if habitacion_raw:
            habitacion_serializada = _serializar_documento_para_llm(habitacion_raw)
            return {"status": "habitacion_encontrada", "datos_habitacion": habitacion_serializada, "mensaje_para_llm": f"Detalles de la habitación '{habitacion_serializada.get('nombre_habitacion', id_habitacion)}' listos."}
        return {"status": "habitacion_no_encontrada", "mensaje_para_llm": f"Habitación ID '{id_habitacion}' no hallada."}
    except Exception as e:
        debug_print(f"🔴 Error en obtener_detalle_habitacion: {str(e)}")
        return {"status": "error_busqueda_db", "mensaje_para_llm": "Problema técnico buscando habitación."}

def obtener_detalle_reserva(id_reserva: str) -> dict:
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a obtener_detalle_reserva(id_reserva='{id_reserva}')")
    if reservas_coll is None: return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo ver reservas ahora (DB)."}
    try:
        reserva_raw = reservas_coll.find_one({"id_reserva": id_reserva})
        if reserva_raw:
            reserva_serializada = _serializar_documento_para_llm(reserva_raw)
            return {"status": "reserva_encontrada", "datos_reserva": reserva_serializada, "mensaje_para_llm": f"Detalles de la reserva '{id_reserva}' listos."}
        return {"status": "reserva_no_encontrada", "mensaje_para_llm": f"Reserva ID '{id_reserva}' no hallada."}
    except Exception as e:
        debug_print(f"🔴 Error en obtener_detalle_reserva: {str(e)}")
        return {"status": "error_busqueda_db", "mensaje_para_llm": "Problema técnico buscando reserva."}

def actualizar_estado_reserva(id_reserva: str, nuevo_estado: str) -> dict:
    debug_print(f"⚙️ Colab/Sammy: LLM llamó a actualizar_estado_reserva(id_reserva='{id_reserva}', nuevo_estado='{nuevo_estado}')")
    if reservas_coll is None: return {"status": "error_conexion_db", "mensaje_para_llm": "No puedo actualizar reservas (DB)."}

    ESTADOS_DE_RESERVA_PERMITIDOS = ["solicitada", "pendiente_confirmacion", "confirmada_por_agente", "confirmada_pagada", "cancelada_por_usuario", "cancelada_por_hotel", "completada", "no_show"]
    if nuevo_estado not in ESTADOS_DE_RESERVA_PERMITIDOS:
        return {"status": "error_validacion_estado", "mensaje_para_llm": f"Estado '{nuevo_estado}' no es válido."}
    try:
        reserva_existente = reservas_coll.find_one({"id_reserva": id_reserva})
        if not reserva_existente: return {"status": "reserva_no_encontrada", "mensaje_para_llm": f"Reserva ID '{id_reserva}' no hallada para actualizar."}
        if reserva_existente.get("estado_reserva") == nuevo_estado:
            return {"status": "sin_cambios_necesarios", "mensaje_para_llm": f"Reserva '{id_reserva}' ya estaba en estado '{nuevo_estado}'."}

        resultado = reservas_coll.update_one(
            {"id_reserva": id_reserva},
            {"$set": {"estado_reserva": nuevo_estado, "fecha_ultima_modificacion": datetime.now()}}
        )
        if resultado.modified_count > 0:
            return {
                "status": "reserva_actualizada_exitosamente",
                "id_reserva_actualizada": id_reserva,
                "nuevo_estado_reserva": nuevo_estado,
                "mensaje_para_llm": f"Reserva '{id_reserva}' actualizada a '{nuevo_estado}'."
            }
        return {"status": "reserva_no_modificada", "mensaje_para_llm": f"Reserva '{id_reserva}' no fue modificada."}
    except Exception as e:
        debug_print(f"🔴 Error en actualizar_estado_reserva: {str(e)}")
        return {"status": "error_actualizacion_db", "mensaje_para_llm": "Problema técnico actualizando reserva."}

# Esta es la lista de todas las funciones que mi LLM (Sammy) podrá utilizar.
# Es importante que los nombres aquí coincidan exactamente con cómo definí las funciones arriba.
herramientas_disponibles_colab = [
    Bienvenida_cliente,
    verificar_o_registrar_usuario,
    registrar_nuevo_usuario,
    listar_servicios_disponibles,
    listar_paquetes_disponibles,
    listar_reservas_por_usuario, # ¡Nueva! Para que Sammy pueda buscar las reservas de un usuario.
    crear_reserva,
    obtener_detalle_paquete,
    obtener_detalle_habitacion,
    obtener_detalle_reserva,
    actualizar_estado_reserva
]

# Celda 5: Función de Inicialización Principal para Colab
# ---------------------------------------------------------------------------
def inicializar_app_colab():
    """
    Esta función es como el "arranque" de mi aplicación en Colab.
    Se encarga de conectar a MongoDB y configurar el modelo Gemini.
    Devuelve True si todo va bien, False si algo falla.
    """
    global client_mongo, db, usuarios_coll, servicios_coll, paquetes_coll, habitaciones_coll, reservas_coll
    global model_gemini, chat_session_sammy, session_state # session_state también es global.

    debug_print("Colab App: Iniciando configuración global...")
    # Reseteo el estado de la sesión cada vez que inicializo, para una conversación limpia.
    session_state = {"bienvenida_dada": False, "ultima_lista_presentada": None, "usuario_identificado": None}

    # Verifico que tenga la URI de MongoDB.
    if not MONGODB_URI:
        print("‼️ Colab App ERROR: MONGODB_ATLAS_URI no configurada en Secrets de Colab.")
        # Si falta, no puedo continuar, así que reseteo todo a None.
        client_mongo = db = usuarios_coll = servicios_coll = paquetes_coll = habitaciones_coll = reservas_coll = None
        model_gemini = chat_session_sammy = None
        return False

    # Intento conectar a MongoDB.
    try:
        client_mongo = MongoClient(MONGODB_URI, server_api=ServerApi('1'))
        client_mongo.admin.command('ping') # Hago un ping para confirmar la conexión.
        db = client_mongo[DB_NAME_PRINCIPAL]
        # Asigno cada colección a su variable global para usarlas fácilmente después.
        usuarios_coll = db[COLL_USUARIOS_NAME]
        servicios_coll = db[COLL_SERVICIOS_NAME]
        paquetes_coll = db[COLL_PAQUETES_NAME]
        habitaciones_coll = db[COLL_HABITACIONES_NAME]
        reservas_coll = db[COLL_RESERVAS_NAME]
        debug_print("✅ Colab App: Conexión a MongoDB establecida y colecciones referenciadas.")
    except Exception as e:
        print(f"‼️ Colab App ERROR conectando a MongoDB: {e}") # Muestro el error si falla.
        client_mongo = db = usuarios_coll = servicios_coll = paquetes_coll = habitaciones_coll = reservas_coll = None
        model_gemini = chat_session_sammy = None
        return False

    # Intento configurar Gemini.
    try:
        if not GEMINI_API_KEY: # Verifico que tenga la API Key de Gemini.
            print("‼️ Colab App ERROR: GEMINI_API_KEY no configurada en Secrets de Colab.")
            model_gemini = chat_session_sammy = None
            return False

        genai.configure(api_key=GEMINI_API_KEY) # Configuro la librería de Gemini.

        # Esta es la instrucción de sistema completa y detallada para Sammy.
        # Le dice cómo comportarse, qué herramientas usar y cómo manejar diferentes situaciones.
        instruccion_sistema_completa = (
            "Eres Sammy, un asistente virtual excepcionalmente amigable, profesional y muy servicial del 'Hotel Paraíso Caribeño', "
            "ubicado en la hermosa ciudad de Santa Marta, Colombia. Tu principal objetivo es ayudar a los clientes con sus consultas "
            "y solicitudes relacionadas con el hotel de manera eficiente, clara y cortés. Mantén siempre un tono positivo y proactivo."
            "\n\n"
            "**Flujo General de la Conversación y Uso de Herramientas:**"
            "\n\n"
            "1.  **Inicio de Conversación:** Cuando un usuario inicie una nueva conversación (ej. 'Hola'), "
            "    SIEMPRE debes comenzar utilizando la herramienta `Bienvenida_cliente`. Esta te guiará sobre qué información solicitar (nombre completo y luego número de documento)."
            "\n\n"
            "2.  **Identificación del Usuario:**"
            "    * Una vez que el usuario te proporcione su nombre y número de documento, utiliza la herramienta `verificar_o_registrar_usuario`."
            "    * **Si `status: 'usuario_encontrado'`:** Saluda al usuario por su nombre y pregúntale cómo puedes ayudarle. Evita repetir el saludo completo de bienvenida si ya se dio."
            "    * **Si `status: 'usuario_no_encontrado_pedir_nombre'`:** Pide el nombre completo."
            "    * **Si `status: 'usuario_no_encontrado_proceder_registro'`:** Informa al usuario que lo vas a registrar (ya que tienes nombre y documento) y llama a `registrar_nuevo_usuario`."
            "\n\n"
            "3.  **Consultas de Información:**"
            "    * **Para paquetes de alojamiento:** Si el usuario pregunta 'qué paquetes hay', 'opciones de paquetes', o similar, usa `listar_paquetes_disponibles`. Para detalles de uno específico, usa `obtener_detalle_paquete`."
            "    * **Para habitaciones individuales:** Usa `obtener_detalle_habitacion`."
            "    * **Para servicios adicionales o tours:** Si el usuario pregunta por estos, usa `listar_servicios_disponibles`. Puedes ofrecer filtrar por `tipo_servicio` (ej. 'tour', 'alimentacion'). Presenta la lista numerada (1, 2, 3...) y recuerda que el precio es por persona. Pide que elijan por NÚMERO o ID."
            "    * **Para reservas del usuario:** Si el usuario pregunta por 'mis reservas' y ya está identificado (verifica el estado de sesión), usa `listar_reservas_por_usuario` con su `numero_documento_usuario`. Si no está identificado, primero identifícalo. Si no recuerda un ID específico, esta función le mostrará todas sus reservas y podrá seleccionar una por número o ID."
            "    * **Para detalles de una reserva específica:** Usa `obtener_detalle_reserva` con el `id_reserva`."
            "\n\n"
            "4.  **Crear una Nueva Reserva:**"
            "    a.  Guía al usuario para elegir un paquete (usando `listar_paquetes_disponibles` y `obtener_detalle_paquete`)."
            "    b.  PREGUNTA si desea añadir servicios/tours. Si sí, usa `listar_servicios_disponibles`. Presenta opciones numeradas. Pide que elija por NÚMERO o ID."
            "    c.  Recolecta: `id_paquete_reservado`, `fecha_inicio_estadia_str` (AAAA-MM-DD), `fecha_fin_estadia_str` (AAAA-MM-DD), `numero_huespedes`, y lista de `ids_servicios_solicitados`."
            "    d.  ANTES de llamar a `crear_reserva`, resume todos los detalles: paquete (nombre), fechas (N noches), huéspedes, servicios adicionales (nombre y costo estimado total para el grupo, aclarando precio p/p), costo estimado del alojamiento (precio_paquete_noche_pp * noches * huéspedes), y costo TOTAL ESTIMADO de la reserva. Pide confirmación."
            "    e.  Si confirma, llama a `crear_reserva`."
            "    f.  Informa el resultado, incluyendo el ID de la reserva y el desglose de costos, enfatizando que es 'solicitud' pendiente de confirmación."
            "\n\n"
            "5.  **Actualizar Estado de una Reserva:** Usa `actualizar_estado_reserva`. Pide confirmación."
            "\n\n"
            "**Principios Generales:**"
            "    * Espera resultados de herramientas. No supongas."
            "    * Maneja errores amigablemente."
            "    * Recolecta parámetros ANTES de llamar herramientas."
            "    * Si el usuario da un NÚMERO para seleccionar de una lista que presentaste, intenta identificar el ID correspondiente. Si no estás seguro, PIDE CONFIRMACIÓN del nombre o ID del ítem antes de usarlo en otra herramienta."
            "    * Sé claro, preciso, conversacional. Prioriza herramientas. No inventes información."
            "    * **MUY IMPORTANTE: Cuando una herramienta te devuelve datos (ej., una lista de paquetes o el resultado de una acción), NUNCA muestres el resultado JSON crudo (como 'tool_outputs' o el diccionario Python) al usuario. En su lugar, USA la información de ese resultado para FORMULAR una respuesta completa y amigable en lenguaje natural para el usuario, presentando los datos de forma legible y útil.**"
        )
        model_gemini = genai.GenerativeModel(
            model_name='gemini-1.5-flash-latest',
            tools=herramientas_disponibles_colab,
            system_instruction=instruccion_sistema_completa
        )
        # Inicio la sesión de chat. enable_automatic_function_calling=False me da más control.
        chat_session_sammy = model_gemini.start_chat(enable_automatic_function_calling=False)
        debug_print("✅ Colab App: Modelo Gemini configurado y sesión de chat lista.")
    except Exception as e:
        print(f"‼️ Colab App ERROR configurando Gemini: {e}") # Muestro el error si falla.
        model_gemini = chat_session_sammy = None
        return False
    return True

# Celda 6: Función para Procesar Mensajes en Colab
# ---------------------------------------------------------------------------
def procesar_mensaje_colab(mensaje_usuario_actual: str) -> str:
    """
    Esta es la función principal que maneja la conversación.
    Envía el mensaje del usuario a Gemini, gestiona las llamadas a herramientas
    y devuelve la respuesta final de Sammy.
    """
    global chat_session_sammy, session_state

    if not model_gemini: # Verifico si el modelo de Gemini está listo.
        return "Lo siento, el asistente no está disponible en este momento (error de modelo)."

    # Si la sesión de chat no existe (ej. primera vez o si se perdió), intento crearla/reiniciarla.
    if not chat_session_sammy:
        try:
            chat_session_sammy = model_gemini.start_chat(enable_automatic_function_calling=False)
            # Reseteo el estado de la sesión para una conversación limpia.
            session_state = {"bienvenida_dada": False, "ultima_lista_presentada": None, "usuario_identificado": None}
            debug_print("✅ Colab/Sammy: Nueva sesión de chat creada.")
        except Exception as e_chat_restart:
            debug_print(f"🔴 Colab/Sammy: No se pudo reiniciar la sesión de chat: {e_chat_restart}")
            return "Lo siento, estoy teniendo problemas para iniciar nuestra conversación. Por favor, intenta más tarde."

    debug_print(f"\n👤 Tú (Colab) envió: {mensaje_usuario_actual}") # Log del mensaje del usuario.
    current_response_from_llm = None # Para guardar la respuesta actual del LLM.

    try:
        # Lógica para mapear una selección numérica del usuario a un ID, si aplica.
        # Esto es para cuando Sammy presenta una lista numerada y el usuario responde con un número.
        if session_state.get("ultima_lista_presentada") and mensaje_usuario_actual.strip().isdigit():
            lista_info = session_state["ultima_lista_presentada"]
            try:
                indice_seleccionado = int(mensaje_usuario_actual.strip()) - 1 # Convierto a índice base 0.

                if 0 <= indice_seleccionado < len(lista_info["items"]):
                    item_seleccionado = lista_info["items"][indice_seleccionado]
                    id_campo_map = {"paquetes": "id_paquete", "servicios": "id_servicio", "reservas": "id_reserva"}
                    id_campo = id_campo_map.get(lista_info["tipo"])

                    if id_campo and id_campo in item_seleccionado:
                        id_seleccionado = item_seleccionado.get(id_campo)
                        nombre_item = item_seleccionado.get("nombre_paquete") or item_seleccionado.get("nombre_servicio") or item_seleccionado.get("id_reserva")

                        # Modifico el mensaje del usuario para que el LLM entienda la selección por ID.
                        mensaje_usuario_actual = f"He seleccionado el ítem con ID '{id_seleccionado}' ({nombre_item}). Por favor, procede con esto."
                        debug_print(f"ℹ️ Colab/Sammy: Usuario seleccionó por número, mapeado a: {mensaje_usuario_actual}")
                        session_state["ultima_lista_presentada"] = None # Limpio la lista después de la selección.
                    else:
                        debug_print(f"⚠️ Colab/Sammy: No se pudo encontrar el campo ID ('{id_campo}') en el item seleccionado.")
                else:
                    debug_print(f"⚠️ Colab/Sammy: Selección numérica '{mensaje_usuario_actual}' fuera de rango.")
            except ValueError: # Si la entrada no era un número válido.
                debug_print(f"⚠️ Colab/Sammy: No se pudo convertir la entrada numérica '{mensaje_usuario_actual}'.")

        # Envío el mensaje (original o modificado) al LLM.
        current_response_from_llm = chat_session_sammy.send_message(mensaje_usuario_actual)

        # Bucle para manejar el flujo de "function calling".
        # El LLM puede decidir llamar a varias funciones seguidas si lo necesita.
        while True:
            # Accedo de forma segura a la posible llamada a función en la respuesta del LLM.
            candidate = current_response_from_llm.candidates[0] if current_response_from_llm.candidates and len(current_response_from_llm.candidates) > 0 else None
            if not candidate or not candidate.content or not candidate.content.parts:
                debug_print("🔴 Colab/Sammy: Respuesta inesperada del LLM sin candidatos o partes válidas.")
                break # Salgo del bucle si la estructura de respuesta no es la esperada.

            part_with_potential_call = candidate.content.parts[0]
            function_call = part_with_potential_call.function_call if hasattr(part_with_potential_call, 'function_call') and part_with_potential_call.function_call.name else None

            if not function_call: # Si no hay más llamadas a función, salgo del bucle.
                break

            function_name = function_call.name
            function_args_dict = dict(function_call.args) # Convierto los argumentos a un diccionario.

            # Lógica para añadir 'nombre_completo' si el usuario ya está identificado
            # y el LLM llama a 'verificar_o_registrar_usuario' solo con el documento.
            if function_name == "verificar_o_registrar_usuario" and \
               session_state.get("usuario_identificado") and \
               "numero_documento" in function_args_dict and \
               not function_args_dict.get("nombre_completo"):
                usuario_identificado_data = session_state["usuario_identificado"]
                function_args_dict["nombre_completo"] = usuario_identificado_data.get("nombre_usuario") or usuario_identificado_data.get("nombrre_usuario")

            args_str = f" con args: {function_args_dict}" if function_args_dict else ""
            debug_print(f"⚙️ Colab/Sammy: LLM quiere llamar a: {function_name}{args_str}")

            # Verifico si la función solicitada por el LLM existe en mi código y es llamable.
            if function_name in globals() and callable(globals()[function_name]):
                funcion_a_llamar = globals()[function_name]
                resultado_funcion_dict = {} # Inicializo el resultado.
                try:
                    # Ejecuto la función herramienta con los argumentos que me dio el LLM.
                    resultado_funcion_dict = funcion_a_llamar(**function_args_dict)
                except Exception as e_func:
                    debug_print(f"🔴 Error al ejecutar la función '{function_name}' desde Colab: {e_func}")
                    resultado_funcion_dict = {"error_interno_python": f"Al ejecutar {function_name}: {str(e_func)}"}

                debug_print(f"➡️ Resultado de '{function_name}' (Colab): {resultado_funcion_dict}")

                # Envío el resultado de mi función de vuelta al LLM.
                function_response_part_to_send = glm.Part(
                    function_response=glm.FunctionResponse(name=function_name, response=resultado_funcion_dict)
                )
                current_response_from_llm = chat_session_sammy.send_message(content=function_response_part_to_send) # Actualizo la respuesta actual.
            else: # Si la función que el LLM quiere llamar no la tengo definida.
                debug_print(f"🔴 Error: Herramienta '{function_name}' no encontrada en Colab.")
                error_payload = {"error_herramienta_no_encontrada": f"La herramienta solicitada '{function_name}' no está implementada."}
                function_response_part_error_to_send = glm.Part(
                    function_response=glm.FunctionResponse(name=function_name, response=error_payload)
                )
                current_response_from_llm = chat_session_sammy.send_message(content=function_response_part_error_to_send)

        # Después del bucle (cuando ya no hay más llamadas a función), la respuesta final debería ser texto.
        # Extraigo el texto de todas las partes de la respuesta del candidato.
        final_text_response = ""
        if current_response_from_llm and current_response_from_llm.candidates:
            for part in current_response_from_llm.candidates[0].content.parts:
                # Me aseguro de que la parte sea texto y no una FunctionCall residual.
                if hasattr(part, 'text') and part.text:
                    final_text_response += part.text
                elif hasattr(part, 'function_call') and part.function_call.name:
                    debug_print(f"⚠️ Colab/Sammy: Se encontró una FunctionCall inesperada ('{part.function_call.name}') al obtener la respuesta de texto final.")

        final_text_response = final_text_response.strip() # Limpio espacios extra.

        if final_text_response:
            return final_text_response
        else: # Si no hay texto, puede ser un error o una respuesta bloqueada.
            debug_print("🔴 Colab/Sammy: LLM no proporcionó una respuesta de texto final legible o la respuesta estaba vacía.")
            if current_response_from_llm and hasattr(current_response_from_llm, 'prompt_feedback') and current_response_from_llm.prompt_feedback.block_reason:
                 return f"Lo siento, no pude procesar completamente tu solicitud debido a: {current_response_from_llm.prompt_feedback.block_reason_message or current_response_from_llm.prompt_feedback.block_reason}"
            return "Entendido. ¿Hay algo más en lo que pueda ayudarte o necesitas que aclare algo?" # Mensaje genérico.

    except Exception as e_convo:
        debug_print(f"🔴 Error general en procesar_mensaje_colab: {e_convo}")
        error_message_from_llm = "Lo siento, tuve un problema procesando tu solicitud general en este momento."
        # Intento dar una respuesta de error más informativa si es posible.
        if current_response_from_llm and current_response_from_llm.candidates and current_response_from_llm.candidates[0].finish_reason != 1: # 1 es 'STOP'
             error_message_from_llm = f"El asistente no pudo completar la respuesta (Razón: {current_response_from_llm.candidates[0].finish_reason}). Intenta de nuevo."
        elif current_response_from_llm and hasattr(current_response_from_llm, 'prompt_feedback') and current_response_from_llm.prompt_feedback.block_reason:
             error_message_from_llm = f"Tu solicitud fue bloqueada por: {current_response_from_llm.prompt_feedback.block_reason_message or current_response_from_llm.prompt_feedback.block_reason}"

        if current_response_from_llm and current_response_from_llm.candidates and current_response_from_llm.candidates[0].content:
             debug_print(f"   Última respuesta del LLM (partes): {current_response_from_llm.candidates[0].content.parts}")
        return error_message_from_llm

# Celda 7: Interfaz Gráfica con ipywidgets y Lógica de Chat
# ---------------------------------------------------------------------------
import html

print("\n--- Configurando Interfaz Gráfica para Sammy ---")

chat_html_style = """
<style>
    .chat-message { margin-bottom: 10px; padding: 8px 12px; border-radius: 12px; max-width: 85%; word-wrap: break-word; line-height: 1.4; font-size: 0.95em; box-shadow: 0 1px 2px rgba(0,0,0,0.1); color: #000000 !important; }
    .user-message { background-color: #DCF8C6; margin-left: auto; text-align: left; align-self: flex-end; border: 1px solid #c5e7b3;}
    .user-message strong { color: #075E54; }
    .sammy-message { background-color: #E9E9EB; margin-right: auto; text-align: left; align-self: flex-start; border: 1px solid #dcdcdc;}
    .sammy-message strong { color: #1F4E79; }
    .error-message { background-color: #FFDDEE; color: #D8000C !important; border: 1px solid #FFBABA; align-self: flex-start;}
    .error-message strong { color: #B00020; }
    .chat-output-container { border: 1px solid #AAB8C2; padding: 15px; height: 450px; overflow-y: auto; display: flex; flex-direction: column-reverse; background-color: #ffffff; border-radius: 8px; }
    .chat-messages-inner { display: flex; flex-direction: column; }
    .chat-messages-inner strong { font-weight: 600; }
    .chat-messages-inner i { color: #555; }
    .input-box { margin-top: 10px; }
</style>
"""
display(HTML(chat_html_style))

chat_output_html = widgets.HTML(value="<div class='chat-messages-inner'></div>", layout=widgets.Layout(height='450px', border='1px solid #ccc', overflow_y='auto', width='100%', padding='5px'))
user_input_field = widgets.Text(value='', placeholder='Escribe tu mensaje a Sammy aquí...', description='', disabled=True, layout=widgets.Layout(width='calc(100% - 105px)'))
send_button = widgets.Button(description='Enviar', disabled=True, button_style='info', tooltip='Enviar mensaje a Sammy', icon='paper-plane', layout=widgets.Layout(width='95px'))

conversation_history_html_ui = []
is_processing_message_ui = False

def add_message_to_ui(raw_text: str, sender_class: str):
    global conversation_history_html_ui # Necesario para modificar la lista global
    prefix = ""
    content_to_display = raw_text
    if raw_text.startswith("<strong>Tú:</strong>"):
        prefix = "<strong>Tú:</strong> "
        message_content = raw_text.replace(prefix, "").strip()
        content_to_display = html.escape(message_content).replace('\n', '<br>')
    elif raw_text.startswith("<strong>🤖 Sammy:</strong>"):
        prefix = "<strong>🤖 Sammy:</strong> "
        message_content = raw_text.replace(prefix, "").strip()
        content_to_display = message_content.replace('\n', '<br>')
    elif raw_text.startswith("<strong>Sistema:</strong>"):
        prefix = "<strong>Sistema:</strong> "
        message_content = raw_text.replace(prefix, "").strip()
        content_to_display = html.escape(message_content).replace('\n', '<br>')
    else:
        content_to_display = html.escape(raw_text).replace('\n', '<br>')

    full_message_html = f"{prefix}{content_to_display}"
    conversation_history_html_ui.insert(0, f"<div class='chat-message {sender_class}'>{full_message_html}</div>")
    chat_output_html.value = f"<div class='chat-messages-inner'>{''.join(conversation_history_html_ui)}</div>"

def on_send_button_clicked_ui(b=None):
    global is_processing_message_ui, conversation_history_html_ui # Declarar global aquí
    if is_processing_message_ui: return

    user_message = user_input_field.value
    if not user_message.strip(): return

    add_message_to_ui(f"<strong>Tú:</strong> {user_message}", "user-message")
    current_input = user_input_field.value
    user_input_field.value = ''

    is_processing_message_ui = True
    original_button_description = send_button.description
    original_button_icon = send_button.icon
    user_input_field.disabled = True
    send_button.disabled = True
    send_button.description = "..."
    send_button.icon = "spinner"

    add_message_to_ui("<strong>🤖 Sammy:</strong> <i>Está pensando...</i>", "sammy-message")

    try:
        sammy_response = procesar_mensaje_colab(current_input)

        # Quitar el mensaje de "pensando..." de forma segura
        conversation_history_html_ui = [msg for msg in conversation_history_html_ui if "<i>Está pensando...</i>" not in msg]
        chat_output_html.value = f"<div class='chat-messages-inner'>{''.join(conversation_history_html_ui)}</div>" # Actualizar UI

        add_message_to_ui(f"<strong>🤖 Sammy:</strong> {sammy_response}", "sammy-message")

    except Exception as e:
        conversation_history_html_ui = [msg for msg in conversation_history_html_ui if "<i>Está pensando...</i>" not in msg]
        chat_output_html.value = f"<div class='chat-messages-inner'>{''.join(conversation_history_html_ui)}</div>" # Actualizar UI
        add_message_to_ui(f"<strong>Sistema:</strong> Error al procesar: {str(e)}", "error-message")
        debug_print(f"🔴 Error en UI durante procesar_mensaje_colab: {e}")
    finally:
        user_input_field.disabled = False
        send_button.disabled = False
        send_button.description = original_button_description
        send_button.icon = original_button_icon
        is_processing_message_ui = False

send_button.on_click(on_send_button_clicked_ui)
user_input_field.on_submit(on_send_button_clicked_ui)

print("\n--- Iniciando Asistente Sammy para Hotel Paraíso Caribeño (Colab con UI) ---")

if inicializar_app_colab():
    print("\n✅ ¡Todo listo! Puedes empezar a chatear con Sammy usando la interfaz de abajo.")

    user_input_field.disabled = False
    send_button.disabled = False

    add_message_to_ui("<strong>🤖 Sammy:</strong> <i>Iniciando conversación...</i>", "sammy-message")

    try:
        initial_bot_response = procesar_mensaje_colab("Hola Sammy, por favor inicia la bienvenida.")
        conversation_history_html_ui = [msg for msg in conversation_history_html_ui if "<i>Iniciando conversación...</i>" not in msg]
        chat_output_html.value = f"<div class='chat-messages-inner'>{''.join(conversation_history_html_ui)}</div>" # Actualizar UI

        if initial_bot_response:
            add_message_to_ui(f"<strong>🤖 Sammy:</strong> {initial_bot_response}", "sammy-message")
        else:
            add_message_to_ui("<strong>Sistema:</strong> No se pudo obtener el saludo inicial de Sammy.", "error-message")
    except NameError as ne:
        conversation_history_html_ui = [msg for msg in conversation_history_html_ui if "<i>Iniciando conversación...</i>" not in msg]
        chat_output_html.value = f"<div class='chat-messages-inner'>{''.join(conversation_history_html_ui)}</div>" # Actualizar UI
        add_message_to_ui(f"<strong>Sistema:</strong> Error de configuración: {str(ne)}. Asegúrate de ejecutar todas las celdas.", "error-message")
        debug_print(f"🔴 Error en UI durante la bienvenida inicial (NameError): {ne}")
    except Exception as e_init_chat:
        conversation_history_html_ui = [msg for msg in conversation_history_html_ui if "<i>Iniciando conversación...</i>" not in msg]
        chat_output_html.value = f"<div class='chat-messages-inner'>{''.join(conversation_history_html_ui)}</div>" # Actualizar UI
        add_message_to_ui(f"<strong>Sistema:</strong> Error al iniciar el chat: {str(e_init_chat)}", "error-message")
        debug_print(f"🔴 Error en UI durante la bienvenida inicial: {e_init_chat}")

    input_area = widgets.HBox([user_input_field, send_button])
    display(chat_output_html, input_area)
else:
    print("\n🔴 FALLÓ LA INICIALIZACIÓN. No se puede iniciar el chat. Revisa los mensajes de error de las celdas anteriores.")